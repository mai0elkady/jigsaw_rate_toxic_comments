{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2hmAXaZ9aQV",
        "outputId": "67d8cb19-0d7d-4895-f2b8-99a4378daf4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = \"/content/drive/MyDrive/UdacityND/project_caps/\""
      ],
      "metadata": {
        "id": "f9znaieKbpst"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "doZr_y2c9dI4"
      },
      "outputs": [],
      "source": [
        "# either use the unclean\n",
        "jigsaw_path = main_path+\"train.csv\"\n",
        "val_data_path = main_path+\"validation_data.csv\"\n",
        "comments_to_score_path = main_path+\"comments_to_score.csv\"\n",
        "ruddit_path = main_path+\"ruddit_with_text.csv\"\n",
        "new_data_path = main_path+\"train_data_version2.csv\"\n",
        "\n",
        "# or clean data\n",
        "jigsaw_path = main_path+\"jigsaw.csv\"\n",
        "val_data_path = main_path+\"val_data.csv\"\n",
        "comments_to_score_path = main_path+\"new_comments.csv\"\n",
        "ruddit_path = main_path+\"ruddit.csv\"\n",
        "new_data_path = main_path+\"new_toxic.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMvpbGle9gOl",
        "outputId": "f0597ef8-982d-494a-fb60-6cf9377e5f41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "random.seed(123)\n",
        "import sys\n",
        "from scipy.stats import rankdata\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pickle\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mOpuvOL8gQ3v"
      },
      "outputs": [],
      "source": [
        "#Read in the data # ref: https://www.kaggle.com/andrej0marinchenko/best-score-0-856-jigsaw-for-beginners\n",
        "def read_jigsaw_toxic_data(train_df_path,  fold_num, use_folds = False):\n",
        "  '''\n",
        "  Read in the toxic jigsaw data\n",
        "  \n",
        "  Inputs:\n",
        "    train_df_path -> path of the file to read\n",
        "    fold_num -> indicates the number of folds if they are usen\n",
        "    use_folds -> whehter to use folds or not\n",
        "  Output: \n",
        "    balanced_df -> a balanced df that has 3 coloumns, comment_text is the text of the comment, toxic_vs_not whether the comment is toxic or not,\n",
        "  y the score of the toxicity calculated\n",
        "  '''\n",
        "  df = pd.read_csv(train_df_path)\n",
        "  df = df.dropna()\n",
        "  count_toxic = df['toxic_vs_not'].value_counts()[1]\n",
        "\n",
        "  if(use_folds==False):\n",
        "    balanced_df = pd.concat([df[df.y>0].sample(frac=1, random_state = 10*(fold_num+1)),df[df.y==0].sample(int(count_toxic), random_state = 10*(fold_num+1))], axis=0)\n",
        "  else:\n",
        "    balanced_df = pd.concat([df[df.y>0].sample(frac=0.8, random_state = 10*(fold_num+1)),df[df.y==0].sample(int(0.8*count_toxic), random_state = 10*(fold_num+1))], axis=0)\n",
        "  fig2, ax2 = plt.subplots()\n",
        "  ax2 = sns.histplot(data=balanced_df,x='y',bins = 10, binwidth=0.08)\n",
        "  \n",
        "  return balanced_df[['comment_text','toxic_vs_not','y']]\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lel8po_c-3ne"
      },
      "outputs": [],
      "source": [
        "# ref: https://www.kaggle.com/andrej0marinchenko/best-score-0-856-jigsaw-for-beginners\n",
        "def read_ruddit_toxic_data(ruddit_path, fold_num = 0, use_folds = False):\n",
        "  '''\n",
        "  Read in ruddit toxic data\n",
        "  \n",
        "  Inputs:\n",
        "    ruddit_path  ->  path of the file to read\n",
        "    fold_num -> indicates the number of folds if they are usen\n",
        "    use_folds -> whehter to use folds or not\n",
        "  \n",
        "  Output: \n",
        "    df  -> a df that has 2 coloumns, comment_text is the text of the comment,\n",
        "  y the score of the toxicity calculated\n",
        "  '''\n",
        "  df = pd.read_csv(ruddit_path)\n",
        "  df = df.dropna()\n",
        "  if(use_folds==True):\n",
        "    df = df.sample(frac=0.8, random_state = 10*(fold_num+1))\n",
        "  df.y.hist()\n",
        "  return df[['comment_text','y']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S3oj55LwIBI-"
      },
      "outputs": [],
      "source": [
        "def read_new_toxic_data(new_path, fold_num = 0, use_folds = False):\n",
        "  '''\n",
        "  Read in alternative jigsaw data\n",
        "  \n",
        "  Inputs:\n",
        "    new_path -> path of the file to read\n",
        "    fold_num -> indicates the number of folds if they are usen\n",
        "    use_folds -> whehter to use folds or not\n",
        "  \n",
        "  Output: \n",
        "    df  -> a balanced df that has 2 coloumns, comment_text is the text of the comment,\n",
        "  y the score of the toxicity calculated\n",
        "  '''\n",
        "  df = pd.read_csv(new_path)\n",
        "  df = df.dropna()\n",
        "  if(use_folds==True):\n",
        "    df = df.sample(frac=0.8, random_state = 10*(fold_num+1))\n",
        "  df.y.hist()\n",
        "  return df[['comment_text','y']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2whx-l3h_45Q"
      },
      "outputs": [],
      "source": [
        "def eval_on_val_data(val_data_path,  model):\n",
        "  '''\n",
        "  Loads validation data from val_data_path that contains \n",
        "  2 rows of comments rated as more toxic and less toxic\n",
        "  this can be used as an indication of how good our actual\n",
        "  model will be in the real test case\n",
        "  \n",
        "  Inputs:\n",
        "    val_data_path -> is the path of the validation data\n",
        "    model -> is the model we use for prediction\n",
        "  \n",
        "  Outputs:\n",
        "    acc -> the average number of times we correctly predictied which comments are more toxic than the others\n",
        "    acc_ranks -> is another accuracy metric that I used that uses rank data on the scores to make sure than \n",
        "    no two scores are the same\n",
        "  '''\n",
        "  all_val_data = pd.read_csv(val_data_path)\n",
        "  all_val_data = all_val_data.dropna()\n",
        "  less_toxic_data = all_val_data['less_toxic']\n",
        "  more_toxic_data = all_val_data['more_toxic']\n",
        "  less_toxic_scores = model.predict(less_toxic_data)\n",
        "  less_toxic_scores_ranks = rankdata( less_toxic_scores, method='ordinal')\n",
        "\n",
        "  more_toxic_scores = model.predict(more_toxic_data)\n",
        "  more_toxic_scores_ranks = rankdata( more_toxic_scores, method='ordinal')\n",
        "\n",
        "  correctness = np.where(more_toxic_scores > less_toxic_scores, 1, 0)\n",
        "  correctness_ranks = np.where(more_toxic_scores_ranks > less_toxic_scores_ranks, 1, 0)\n",
        "  acc = np.mean(correctness)\n",
        "  acc_ranks = np.mean(correctness_ranks)\n",
        "  \n",
        "  return acc, acc_ranks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rMgkfshyAi_d"
      },
      "outputs": [],
      "source": [
        "def train_model(model, n_folds, data_path, val_data_path, dataname):\n",
        "  '''\n",
        "  Train the model on the data in data_path\n",
        "  \n",
        "  Inputs:\n",
        "    model -> the model or pipleine to be trained\n",
        "    n_folds -> the number of folds to use\n",
        "    data_path -> the path of the data to load\n",
        "    dataname -> the name of the data\n",
        "  Output:\n",
        "    the average accuracy across folds for the training data, and the same for the evaluation on the validation data\n",
        "  '''\n",
        "  if(n_folds>1):\n",
        "    use_folds = True\n",
        "  else:\n",
        "    use_folds = False\n",
        "  model_scores = []\n",
        "  accs = []\n",
        "  accs_ranks = []\n",
        "  for i in range(0,n_folds):\n",
        "    if(dataname=='jigsaw'):\n",
        "      df = read_jigsaw_toxic_data(data_path, i, use_folds)\n",
        "    elif(dataname=='new_data'):\n",
        "      df = read_new_toxic_data(data_path, i, use_folds)\n",
        "    elif(dataname=='ruddit'):\n",
        "      df = read_ruddit_toxic_data(data_path, i, use_folds)\n",
        "    else:\n",
        "      print(\"Wrong data name\") \n",
        "    \n",
        "    model.fit(df['comment_text'], df['y'])\n",
        "    model_score = model.score(df['comment_text'], df['y'])\n",
        "    model_scores.append(model_score)\n",
        "    acc,acc_ranks = eval_on_val_data(val_data_path,  model)\n",
        "    accs.append(acc)\n",
        "    accs_ranks.append(acc_ranks)\n",
        "  return sum(model_scores)/n_folds, sum(accs)/n_folds, sum(accs_ranks)/n_folds, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y82idnLSB6pG"
      },
      "outputs": [],
      "source": [
        "def train_pipeline(train_data_path, val_data_path, dataname, n_folds):\n",
        "  '''\n",
        "  Basically performing a manual gridsearch, but it allows me more flexibility when choosing the evaluation method\n",
        "  \n",
        "  Inputs:\n",
        "     train_data_path -> the path of the data to train on\n",
        "     val_data_path -> the path of the validation data\n",
        "     dataname   -> the name of the data\n",
        "     n_folds  ->  the number of folds\n",
        "  \n",
        "  Outputs:\n",
        "    best_val_acc -> a float indicating the best validation accuracy  \n",
        "    best_ngram -> a tuple of ngram values that corresponded to the best_val_acc\n",
        "    best_analyzer -> a str of the best analyzer that corresponds to the best_val_acc \n",
        "    best_alpha -> a float indicating the alpha value that corresponds to the best_val_acc \n",
        "  '''\n",
        "  analyzer_values = ['word','char_wb']\n",
        "  ngram_ranges = [(1,1),(3,5)]\n",
        "  alphas = [0.01, 0.1, 1, 10, 100]\n",
        "  best_val_acc = 0\n",
        "  \n",
        "  for analyzer in analyzer_values:\n",
        "    for ngram_range in ngram_ranges:\n",
        "      \n",
        "      if(ngram_range==(1,1) and analyzer=='char_wb'):\n",
        "        ngram_range = (5,10)\n",
        "    \n",
        "      for alpha in alphas:\n",
        "        \n",
        "          pipeline = Pipeline([\n",
        "                  ('tfidf', TfidfVectorizer(min_df = 10, analyzer=analyzer, ngram_range=ngram_range)),\n",
        "                  ('clf', Ridge(random_state=42, alpha=alpha))\n",
        "          ])\n",
        "          avg_train_acc, avg_val_acc, avg_val_acc_ranks, model = train_model(pipeline, n_folds, train_data_path, val_data_path, dataname)\n",
        "          print(\"For analyzer = \"+analyzer+ \" ngram range is \"+str(ngram_range)+\" alpha is \"+str(alpha)+\" training r2: \"+str(avg_train_acc)+ \", val_acc: \"+str(avg_val_acc)+\" val ranks acc: \"+str(avg_val_acc_ranks))\n",
        "          if(avg_val_acc > best_val_acc):\n",
        "            best_val_acc = avg_val_acc\n",
        "            best_ngram = ngram_range\n",
        "            best_analyzer = analyzer\n",
        "            best_alpha = alpha\n",
        "            best_model = model\n",
        "            \n",
        "\n",
        "    \n",
        "  return best_val_acc, best_ngram, best_analyzer, best_alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrKrUwj5YOIN"
      },
      "outputs": [],
      "source": [
        "best_ridge_jigsaw = train_pipeline(jigsaw_path, val_data_path,  \"jigsaw\", 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO3q1n0d7YPw"
      },
      "outputs": [],
      "source": [
        "best_ridge_jigsaw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj-Mej-JJYU7"
      },
      "outputs": [],
      "source": [
        "best_ridge_ruddit = train_pipeline(ruddit_path, val_data_path,  \"ruddit\", 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_ridge_ruddit"
      ],
      "metadata": {
        "id": "6eeUyPFIhnMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex0o8P5Vn4QB"
      },
      "outputs": [],
      "source": [
        "best_ridge_new_data = train_pipeline(new_data_path, val_data_path,  \"new_data\", 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_ridge_new_data"
      ],
      "metadata": {
        "id": "Ow2UYtBehp6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9E2Rkv6C6w-"
      },
      "outputs": [],
      "source": [
        "def train_final(train_data_path, val_data_path, dataname, n_folds, analyzer, ngram_range, alpha):\n",
        "  '''\n",
        "  Uses parameters obtained from previous trains to train a model on the full dataset\n",
        "\n",
        "  Inputs:\n",
        "     train_data_path -> the path of the data to train on\n",
        "     val_data_path -> the path of the validation data\n",
        "     dataname   -> the name of the data\n",
        "     n_folds  ->  the number of folds\n",
        "     analyzer -> the type of tfidf analyzer to use\n",
        "     ngram_range -> the ngram range to use with the tfidf analyzer\n",
        "     alpha -> the regularization value to use with the ridge regression (will be set to 0 for linear)\n",
        "  \n",
        "  Outputs:\n",
        "    model -> The final trained model \n",
        "\n",
        "  '''\n",
        "\n",
        "  pipeline = Pipeline([\n",
        "          ('tfidf', TfidfVectorizer(min_df = 10, analyzer=analyzer, ngram_range=ngram_range)),\n",
        "          ('clf', Ridge(random_state=42, alpha=alpha))\n",
        "  ])\n",
        "  avg_train_acc, avg_val_acc, avg_val_acc_ranks, model = train_model(pipeline, n_folds, train_data_path, val_data_path, dataname)\n",
        "\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lAGeEDnE_1m"
      },
      "outputs": [],
      "source": [
        "def predict_on_new_comments(path_to_comments, model_jigsaw, model_ruddit, model_new_data, ws):\n",
        "  '''\n",
        "  Use models trained on different datasets to obtain the final prediction for the dataset that we need to score\n",
        "  in the kaggle competition\n",
        "\n",
        "  Inputs:\n",
        "     path_to_comments -> the path of the new comments that we need to score\n",
        "     model_jigsaw -> the trained model on jigsaw data\n",
        "     model_ruddit  -> the trained model on ruddit data\n",
        "     model_new_data   -> the trained model on alternative jigsaw data\n",
        "     ws  ->  the weights of each model\n",
        "     \n",
        "\n",
        "  '''\n",
        "  new_data = pd.read_csv(path_to_comments)\n",
        "  comment_ids = new_data['comment_id'].to_numpy()\n",
        "  scores = ws[0]*model_jigsaw.predict(new_data['text']) + ws[1]*model_ruddit.predict(new_data['text']) + ws[2]*model_new_data.predict(new_data['text'])\n",
        "  df = pd.DataFrame({'comment_id':comment_ids, 'score':scores})\n",
        "  df.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtWr-Mr0onbP"
      },
      "outputs": [],
      "source": [
        "# ref: https://stackoverflow.com/questions/5563808/how-to-generate-three-random-numbers-whose-sum-is-1\n",
        "# ref: https://www.kaggle.com/sytuannguyen/overfitting-lb-is-easier-than-solving-the-problem\n",
        "# ref: https://www.kaggle.com/andrej0marinchenko/best-score-0-856-jigsaw-for-beginners\n",
        "\n",
        "def ensemble_all_data(best_ruddit, best_new_data, best_jigsaw):\n",
        "  '''\n",
        "  Train models that has best val results on all their data using the best prameters\n",
        "  combine scores to get better results\n",
        "  \n",
        "  Inputs:\n",
        "    best_ruddit -> the best parameters that were obtained from training on ruddit \n",
        "    best_new_data -> the best parameters that were obtained from training on alt jigsaw \n",
        "    best_jigsaw -> the best parameters that were obtained from training on jigsaw\n",
        "  '''\n",
        "  best_val_acc, ngram_range, analyzer, alpha = best_jigsaw\n",
        "  model_jigsaw =  train_final(jigsaw_path, val_data_path,  \"jigsaw\", 1, analyzer, ngram_range, alpha)\n",
        "  best_val_acc, ngram_range, analyzer, alpha = best_ruddit\n",
        "  model_ruddit =  train_final(ruddit_path, val_data_path,  \"ruddit\", 1, analyzer, ngram_range, alpha)\n",
        "  best_val_acc, ngram_range, analyzer, alpha = best_new_data\n",
        "  model_new_data =  train_final(new_data_path, val_data_path,  \"new_data\", 1, analyzer, ngram_range, alpha)\n",
        "\n",
        "  all_val_data = pd.read_csv(val_data_path)\n",
        "  less_toxic_data = all_val_data['less_toxic']\n",
        "  more_toxic_data = all_val_data['more_toxic']\n",
        "  \n",
        "  less_toxic_scores_jigsaw = model_jigsaw.predict(less_toxic_data)\n",
        "  less_toxic_scores_ranks_jigsaw = rankdata( less_toxic_scores_jigsaw, method='ordinal')\n",
        "\n",
        "  more_toxic_scores_jigsaw = model_jigsaw.predict(more_toxic_data)\n",
        "  more_toxic_scores_ranks_jigsaw = rankdata( more_toxic_scores_jigsaw, method='ordinal')\n",
        "\n",
        "\n",
        "  less_toxic_scores_ruddit = model_ruddit.predict(less_toxic_data)\n",
        "  less_toxic_scores_ranks_ruddit = rankdata( less_toxic_scores_ruddit, method='ordinal')\n",
        "\n",
        "  more_toxic_scores_ruddit = model_ruddit.predict(more_toxic_data)\n",
        "  more_toxic_scores_ranks_ruddit = rankdata( more_toxic_scores_ruddit, method='ordinal')\n",
        "\n",
        "  less_toxic_scores_new_data = model_new_data.predict(less_toxic_data)\n",
        "  less_toxic_scores_ranks_new_data = rankdata( less_toxic_scores_new_data, method='ordinal')\n",
        "\n",
        "  more_toxic_scores_new_data = model_new_data.predict(more_toxic_data)\n",
        "  more_toxic_scores_ranks_new_data = rankdata( more_toxic_scores_new_data, method='ordinal')\n",
        "  \n",
        "  best_acc = 0\n",
        "  best_acc_ranks = 0\n",
        "\n",
        "  for i in range(0,500):\n",
        "\n",
        "\n",
        "    a = random.uniform(0, 1)\n",
        "    x = random.uniform(0, a)\n",
        "    y = random.uniform(a, 1)\n",
        "    w1 = x\n",
        "    w2 = y-x\n",
        "    w3 = 1-y\n",
        "    \n",
        "    more_toxic_scores = w1*more_toxic_scores_jigsaw + w2*more_toxic_scores_ruddit + w3*more_toxic_scores_new_data\n",
        "    less_toxic_scores = w1*less_toxic_scores_jigsaw + w2*less_toxic_scores_ruddit + w3*less_toxic_scores_new_data\n",
        "    \n",
        "    more_toxic_scores_ranks = w1*more_toxic_scores_ranks_jigsaw + w2*more_toxic_scores_ranks_ruddit + w3*more_toxic_scores_ranks_new_data\n",
        "    less_toxic_scores_ranks = w1*less_toxic_scores_ranks_jigsaw + w2*less_toxic_scores_ranks_ruddit + w3*less_toxic_scores_ranks_new_data\n",
        "\n",
        "    correctness = np.where(more_toxic_scores > less_toxic_scores, 1, 0)\n",
        "    correctness_ranks = np.where(more_toxic_scores_ranks > less_toxic_scores_ranks, 1, 0)\n",
        "    \n",
        "    acc = np.mean(correctness)\n",
        "    acc_ranks = np.mean(correctness_ranks)\n",
        "    if(acc>best_acc):\n",
        "      best_acc = acc\n",
        "      best_ws_acc = (w1,w2,w3)\n",
        "      \n",
        "    if(acc_ranks>best_acc_ranks):\n",
        "      best_acc_ranks = acc_ranks\n",
        "      best_ws_acc_ranks = (w1,w2,w3)\n",
        "    \n",
        "    print(\"For w1 = \"+str(w1)+\" w2 is \"+str(w2)+\" w3 is \"+str(w3)+ \" val acc is: \"+str(acc)+\" val acc ranks is: \"+str(acc_ranks))\n",
        "  predict_on_new_comments(comments_to_score_path, model_jigsaw, model_ruddit, model_new_data, best_ws_acc)\n",
        "  print(\"Best accuracy on val data was \"+str(best_acc))\n",
        "  print(\"This correponded to these ws\")\n",
        "  print(best_ws_acc)\n",
        "  #print(best_acc_ranks)\n",
        "  #print(best_ws_acc_ranks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_Op7TMLTe51"
      },
      "outputs": [],
      "source": [
        "# I can skip the training before this section, by uncommenting the lines below\n",
        "#best_ridge_new_data = (0.6715048049244942, (1, 1), 'word', 1)\n",
        "#best_ridge_ruddit = (0.6238541251494619, (3, 5), 'char_wb', 0.1)\n",
        "#best_ridge_jigsaw = (0.6795868207785306, (3, 5), 'char_wb', 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWWWhK5HTfh8"
      },
      "outputs": [],
      "source": [
        "ensemble_all_data(best_ridge_new_data, best_ridge_new_data, best_ridge_jigsaw)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "tfidf_and_ridge.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}